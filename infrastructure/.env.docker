# =============================================================================
# GM Agent Docker Configuration
# =============================================================================
# Copy this file to .env.docker and customize for your environment.
# These values are used by docker-compose.yml.
# =============================================================================

# =============================================================================
# LLM Backend Selection
# =============================================================================
# Available backends: ollama, openai, anthropic, openrouter
LLM_BACKEND=ollama

# =============================================================================
# Ollama Settings
# =============================================================================
# URL of the Ollama server (use host.docker.internal for local Ollama on host)
OLLAMA_URL=http://host.docker.internal:11434

# Model to use for generation
OLLAMA_MODEL=qwen3:latest

# =============================================================================
# OpenAI Settings (if using openai backend)
# =============================================================================
# OPENAI_API_KEY=sk-your-api-key-here
# OPENAI_MODEL=gpt-4o-mini
# OPENAI_BASE_URL=https://api.openai.com/v1

# =============================================================================
# Anthropic Settings (if using anthropic backend)
# =============================================================================
# ANTHROPIC_API_KEY=sk-ant-your-api-key-here
# ANTHROPIC_MODEL=claude-haiku

# =============================================================================
# OpenRouter Settings (if using openrouter backend)
# =============================================================================
# OPENROUTER_API_KEY=sk-or-your-api-key-here
# OPENROUTER_MODEL=openai/gpt-4o-mini

# =============================================================================
# Tool Execution Settings
# =============================================================================
# Enable parallel execution of independent MCP tool calls
# Default: false (sequential) - set to true if your LLM backend can handle parallel requests
PARALLEL_TOOL_CALLS=false

# =============================================================================
# Context Settings
# =============================================================================
# Maximum number of recent turns to include in context
MAX_RECENT_TURNS=15

# Maximum estimated tokens for context (used for truncation)
MAX_CONTEXT_TOKENS=8000

# Number of turns between automatic rolling summaries
TURNS_BETWEEN_SUMMARIES=10

# =============================================================================
# API Authentication
# =============================================================================
# Enable JWT authentication for API endpoints
API_AUTH_ENABLED=false

# Secret key for JWT tokens (CHANGE THIS IN PRODUCTION!)
JWT_SECRET_KEY=docker-secret-change-in-production

# Default credentials (CHANGE THESE IN PRODUCTION!)
API_USERNAME=admin
API_PASSWORD=changeme

# =============================================================================
# Redis/Celery Settings (usually no need to change)
# =============================================================================
# These are set automatically by docker-compose.yml
# REDIS_URL=redis://redis:6379/0
# CELERY_BROKER_URL=redis://redis:6379/0
# CELERY_RESULT_BACKEND=redis://redis:6379/0

# =============================================================================
# State Checkpointing
# =============================================================================
# Interval between state checkpoints (seconds)
CHECKPOINT_INTERVAL=30

# How long before a checkpoint is considered stale (seconds)
CHECKPOINT_STALE_THRESHOLD=300

# =============================================================================
# MCP (Model Context Protocol) Settings
# =============================================================================
# MCP execution mode:
# - "local" = Direct in-process server execution (default for API/workers)
# - "remote" = CLI -> API -> Celery -> Workers (used by CLI in Docker)
# Note: docker-compose.yml overrides this to "remote" for CLI service
# MCP_MODE=local

# API URL for remote mode tool execution
# MCP_API_URL=http://api:5000

# =============================================================================
# Logging
# =============================================================================
# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO
